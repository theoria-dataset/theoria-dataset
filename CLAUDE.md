# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Overview

TheorIA (Theoretical Physics Intelligent Anthology) is a curated dataset of theoretical physics equations, derivations, and explanations in structured JSON format. The dataset is designed for training and evaluating machine learning models on physics reasoning.

## Core Architecture

### Data Structure

**Individual Entries**: Each physics result lives in `entries/{result_id}.json` following the schema in `schemas/entry.schema.json`. Entries are self-contained with:
- Formal derivations (step-by-step in AsciiMath)
- Programmatic verification (Python + SymPy)
- Global assumption references
- Dependency tracking between entries

**Global Assumptions**: Centralized in `globals/assumptions.json` following `schemas/assumptions.schema.json`. Three types:
1. **principle**: Core theoretical/mathematical postulates
2. **empirical**: Experimentally established facts and measured constants
3. **approximation**: Validity restrictions and simplifying modeling choices

**Manifest**: `manifest.json` tracks dataset version and metadata.

### Requirements System (Source of Truth)

The JSON Schema files are the **single source of truth** for validation AND documentation:
- `schemas/entry.schema.json` - Entry validation + embedded field guidelines
- `schemas/assumptions.schema.json` - Assumption validation rules

Auto-generated files (DO NOT edit directly):
- `CONTRIBUTING.md` - Generated from schema by `scripts/generate_contributing.py`
- Web form requirements JavaScript - Generated by `scripts/generate_form_requirements.py`

When changing requirements, ONLY edit the schema files, then run:
```bash
docker-compose run --rm theoria-tests python scripts/build_requirements.py
```

## Common Development Commands

### Testing Entries

```bash
# Test all entries (schema + programmatic verification)
make test

# Test specific entry by name (without .json extension)
make test-entry FILE=newtons_laws_of_motion

# Test directly with Python (in Docker)
docker-compose run --rm theoria-tests python scripts/test_entry.py carnot_efficiency

# Schema validation only
docker-compose run --rm theoria-tests python scripts/validate_schema.py entries/your_entry.json

# Programmatic verification only
docker-compose run --rm theoria-tests python scripts/verify_programmatic.py
```

### Building and Validation

```bash
# Full pre-push check (requirements, notebooks, index, schemas, dependencies, tests)
make pre-push

# Rebuild requirements from schema
docker-compose run --rm theoria-tests python scripts/build_requirements.py

# Generate entry index page for docs
docker-compose run --rm theoria-tests python scripts/generate_index.py

# Generate Jupyter notebooks from entries
docker-compose run --rm theoria-tests python scripts/generate_notebooks.py

# Validate all schemas
docker-compose run --rm theoria-tests python scripts/validate_all_schemas.py

# Validate entry dependencies
docker-compose run --rm theoria-tests python scripts/validate_dependencies.py

# Validate assumption usage (check ID conflicts and step references)
docker-compose run --rm theoria-tests python scripts/validate_assumptions_usage.py
```

### ML Dataset Generation

```bash
# Generate ML dataset (reviewed entries only)
python scripts/build_ml_dataset.py

# Include draft entries
python scripts/build_ml_dataset.py --include-drafts

# Test ML dataset script
python scripts/test_ml_dataset.py
```

### Development Environment

```bash
# Run tests in Docker (matches CI exactly)
docker-compose run --rm theoria-tests

# Interactive development shell
docker-compose run --rm theoria-dev

# Rebuild Docker image after dependency changes
docker-compose build
```

## Important Conventions

### Entry IDs and Filenames
- Entry `result_id` must EXACTLY match filename (without .json)
- Use `lowercase_with_underscores` only
- Example: `result_id: "schrodinger_equation"` → `entries/schrodinger_equation.json`

### Mathematical Notation (AsciiMath)
**Critical formatting rules:**
- Fractions with parentheses: `(dr)/(dt)` NOT `dr/dt` (unless single char/digit)
- Derivatives: `(d^2 u)/(d theta^2)` NOT `d^2u/dtheta^2`
- Partial derivatives: `(del u)/(del t)` NOT `∂u/∂t`
- Multi-char subscripts: `N_(sphere)` NOT `N_sphere`
- Avoid "to" in subscripts: Use `N_(all)` NOT `N_total` (renders as arrow)
- Text in equations: `text{constant}` NOT `text(constant)`

### Derivation Quality Standards
1. **Completeness**: Start from first principles or entry dependencies (in `depends_on`), show ALL intermediate steps
2. **Assumptions**: Reference global assumption IDs from `globals/assumptions.json`
3. **Logical independence**: Each assumption should be genuinely independent (not derivable from others)
4. **Verification**: Python code using SymPy that validates EACH step with assert statements
5. **Comments**: Annotate verification code with step numbers (`# Step 2`, `# Steps 4-8`)

### Review Status
- Use `"review_status": "draft"` for new submissions
- Only maintainers set `"review_status": "reviewed"`

## Specialized Scripts

**Schema/Requirements Management:**
- `build_requirements.py` - Regenerates CONTRIBUTING.md and form requirements from schema
- `validate_schema.py` - Validates single entry against schema
- `validate_all_schemas.py` - Validates all entries and global assumptions
- `validate_dependencies.py` - Checks entry dependency graph
- `validate_assumptions_usage.py` - Validates assumption ID uniqueness and step-level usage

**Content Generation:**
- `generate_index.py` - Creates docs/entries_index.html from all entries
- `generate_notebooks.py` - Generates Jupyter notebooks from programmatic_verification
- `generate_form.py` - Web form generation utilities

**Testing:**
- `test_entry.py` - Comprehensive entry testing (schema + verification)
- `verify_programmatic.py` - Runs all programmatic verifications
- `test_ml_dataset.py` - Tests ML dataset generation

**Utilities:**
- `build_ml_dataset.py` - Creates unified ML dataset with resolved assumptions
- `extract_changelog.py` - Extracts version-specific changelog for releases
- `parse_github_issue.py` - Parses contribution issues

## CI/CD

**Automatic Workflows:**
- Entry validation on PR
- Notebook generation on push
- GitHub release creation on version tag (format: `v*`)

**Release Process:**
1. Update CHANGELOG.md following Keep a Changelog format
2. Commit: `git commit -m "Prepare release vX.Y.Z"`
3. Create and push tag: `git tag vX.Y.Z && git push origin vX.Y.Z`
4. GitHub Action automatically creates release with changelog notes

## Environment Details

- **Python**: 3.11.12
- **SymPy**: 1.12.0 (exact version for consistency)
- **Node.js**: 14 with ajv-cli for JSON Schema validation
- **Docker**: Ensures CI/local parity

## Key Principles

1. **One JSON per entry**: Enables parallel contributions and clean version control
2. **Schema as source of truth**: All requirements flow from JSON Schema files
3. **Self-contained entries**: Each entry fully defines symbols, references assumptions
4. **Programmatic verification**: Math correctness guaranteed by executable code
5. **Open collaboration**: CC-BY 4.0 license, contributor attribution required
